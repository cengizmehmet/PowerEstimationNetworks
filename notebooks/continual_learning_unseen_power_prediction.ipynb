{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cengizmehmet/PowerEstimationNetworks/blob/master/notebooks/continual_learning_unseen_power_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Dependencies\n",
        "# ============================================================\n",
        "\n",
        "# --- Standard library & typing ---\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, Iterable, Optional, Union\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# --- Core scientific stack ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Visualisation (diagnostics & analysis) ---\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Machine learning / deep learning ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import (\n",
        "    Input,\n",
        "    Model,\n",
        "    backend as K,\n",
        "    layers,\n",
        "    metrics,\n",
        "    optimizers,\n",
        "    regularizers,\n",
        ")\n",
        "from tensorflow.keras.activations import softplus\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# --- Classical ML utilities ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    MinMaxScaler,\n",
        "    RobustScaler,\n",
        "    Normalizer,\n",
        "    MaxAbsScaler,\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    mean_pinball_loss,\n",
        ")"
      ],
      "metadata": {
        "id": "BYorxMpgC2e-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "byZu6d_HzrAh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Reproducibility\n",
        "# ============================================================\n",
        "# Set random seeds for Python, NumPy, and TensorFlow to ensure\n",
        "# deterministic behaviour across runs (as far as supported).\n",
        "\n",
        "SEED = 42  # arbitrary but fixed for reproducibility\n",
        "\n",
        "def set_global_seed(seed: int) -> None:\n",
        "    \"\"\"Set random seeds for reproducible experiments.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_global_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"\n",
        "    Lightweight utility for loading tabular data, fitting/assigning a Keras tokenizer,\n",
        "    and producing simple sequence-length statistics + padded tokenized sequences.\n",
        "\n",
        "    Notes:\n",
        "        - This class intentionally stores both `data` and `targets` as loaded from CSV.\n",
        "        - Tokenizer-related metadata is tracked in `self.stats`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, target_column: str):\n",
        "        self.data = pd.read_csv(data_path)\n",
        "        self.targets = self.data[target_column]\n",
        "        self.tokenizer = None\n",
        "        self.max_seq_len = None\n",
        "        self.stats: Dict[str, Any] = {}\n",
        "\n",
        "    def initialize_tokenizer(self, data_column: str) -> None:\n",
        "        \"\"\"Create and fit a new tokenizer on the specified text column.\"\"\"\n",
        "        self.tokenizer = Tokenizer()\n",
        "        sentences = self.data[data_column].to_list()\n",
        "        self.tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "        self.stats[\"num_words\"] = len(self.tokenizer.word_index) + 1\n",
        "        self.stats[\"embedding_dim\"] = min(512, self.stats[\"num_words\"])\n",
        "\n",
        "    def set_tokenizer(self, tokenizer: Tokenizer, max_seq_len: int, embedding_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Assign an existing tokenizer and set tokenizer-dependent stats.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: A pre-fit Keras Tokenizer.\n",
        "            max_seq_len: Kept for API compatibility with your notebook (intentionally not stored/used here).\n",
        "            embedding_dim: Desired embedding dimensionality (explicitly set by caller).\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stats[\"num_words\"] = len(self.tokenizer.word_index) + 1\n",
        "        self.stats[\"embedding_dim\"] = embedding_dim\n",
        "\n",
        "    def length_stats(self, sequences: pd.Series) -> None:\n",
        "        \"\"\"\n",
        "        Compute sequence length statistics (tokenized lengths) from raw text.\n",
        "\n",
        "        Stores results into:\n",
        "            stats['max_length'], stats['min_length'], stats['avg_length'], stats['percentile_95']\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Tokenizer is not initialized. Call initialize_tokenizer() or set_tokenizer() first.\")\n",
        "\n",
        "        sentences = sequences.tolist()\n",
        "        tokenized_sentences = self.tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "        sentence_lengths = [len(s) for s in tokenized_sentences if len(s) > 0]\n",
        "        if not sentence_lengths:\n",
        "            # Preserve mechanics: previously this would error on np.max/min.\n",
        "            # Here we raise a clearer message while still failing fast.\n",
        "            raise ValueError(\"All sequences are empty after tokenization; cannot compute length statistics.\")\n",
        "\n",
        "        self.stats[\"max_length\"] = np.max(sentence_lengths)\n",
        "        self.stats[\"min_length\"] = np.min(sentence_lengths)\n",
        "        self.stats[\"avg_length\"] = np.mean(sentence_lengths)\n",
        "        self.stats[\"percentile_95\"] = np.percentile(sentence_lengths, 95)\n",
        "\n",
        "    def resolve_max_seq_len(self, max_seq_len: Union[int, str]) -> int:\n",
        "        \"\"\"\n",
        "        Resolve max sequence length either from an explicit integer or a stats key.\n",
        "\n",
        "        Accepts:\n",
        "            - int: used directly\n",
        "            - str: one of {'min_length', 'max_length', 'avg_length', 'percentile_95'}\n",
        "        \"\"\"\n",
        "        if isinstance(max_seq_len, int):\n",
        "            return max_seq_len\n",
        "        if max_seq_len in self.stats:\n",
        "            return int(self.stats[max_seq_len])\n",
        "\n",
        "        raise ValueError(\n",
        "            \"Invalid max_seq_len value. Choose from \"\n",
        "            \"'min_length', 'max_length', 'avg_length', 'percentile_95', or an integer.\"\n",
        "        )\n",
        "\n",
        "    def pad_tokenize_sequences(\n",
        "        self,\n",
        "        tokenizer: Tokenizer,\n",
        "        sequences: Iterable[str],\n",
        "        max_seq_len: int,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Tokenize sequences and pad them to `max_seq_len` (post-padding).\"\"\"\n",
        "        tokenized_sequences = tokenizer.texts_to_sequences(sequences)\n",
        "        padded_sequences = pad_sequences(tokenized_sequences, maxlen=max_seq_len, padding=\"post\")\n",
        "        return padded_sequences\n",
        "\n",
        "    def print_stats(self, stats: Dict[str, Any]) -> None:\n",
        "        \"\"\"Pretty-print common stats if present.\"\"\"\n",
        "        key_to_label = {\n",
        "            \"num_words\": \"Vocabulary size\",\n",
        "            \"embedding_dim\": \"Embedding dimension\",\n",
        "            \"max_length\": \"Max length\",\n",
        "            \"min_length\": \"Min length\",\n",
        "            \"avg_length\": \"Avg length\",\n",
        "            \"percentile_95\": \"Percentile 95\",\n",
        "        }\n",
        "\n",
        "        for key, label in key_to_label.items():\n",
        "            if key in stats:\n",
        "                print(f\"{label}: {stats[key]}\")"
      ],
      "metadata": {
        "id": "BycNcb0QhqP-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Data paths (user-configurable)\n",
        "# ============================================================\n",
        "# NOTE:\n",
        "# These paths can be freely changed by the user depending on where\n",
        "# the dataset is stored (e.g., local machine, Google Drive, or remote URL).\n",
        "#\n",
        "# Official raw GitHub versions (optional alternatives):\n",
        "#   - Training: https://raw.githubusercontent.com/cengizmehmet/PowerEstimationNetworks/refs/heads/master/data/splits/training.csv\n",
        "#   - Test:     https://raw.githubusercontent.com/cengizmehmet/PowerEstimationNetworks/refs/heads/master/data/splits/test.csv\n",
        "#   - Unknown:  https://raw.githubusercontent.com/cengizmehmet/PowerEstimationNetworks/refs/heads/master/data/splits/unknown.csv\n",
        "\n",
        "training_data_path = \"https://raw.githubusercontent.com/cengizmehmet/PowerEstimationNetworks/refs/heads/master/data/splits/training.csv\"\n",
        "test_data_path     = \"https://raw.githubusercontent.com/cengizmehmet/PowerEstimationNetworks/refs/heads/master/data/splits/test.csv\"\n",
        "unknown_data_path  = \"https://raw.githubusercontent.com/cengizmehmet/PowerEstimationNetworks/refs/heads/master/data/splits/unknown.csv\"\n",
        "\n",
        "DATA_COLUMN = \"Model\"\n",
        "TARGET_COLUMN = \"Avg_Power_Simpson\""
      ],
      "metadata": {
        "id": "cCDlVoY5hqNy"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Training data processing\n",
        "# ============================================================\n",
        "data_processor = DataProcessor(training_data_path, TARGET_COLUMN)\n",
        "data_processor.initialize_tokenizer(DATA_COLUMN)\n",
        "data_processor.length_stats(data_processor.data[DATA_COLUMN])\n",
        "data_processor.print_stats(data_processor.stats)\n",
        "\n",
        "data_processor.max_seq_len = data_processor.resolve_max_seq_len(\"avg_length\")\n",
        "\n",
        "data_sequences = np.array(\n",
        "    data_processor.pad_tokenize_sequences(\n",
        "        tokenizer=data_processor.tokenizer,\n",
        "        sequences=data_processor.data[DATA_COLUMN],\n",
        "        max_seq_len=data_processor.max_seq_len,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "IyPvFk9vhqKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Test data processing (shared tokenizer)\n",
        "# ============================================================\n",
        "test_processor = DataProcessor(test_data_path, TARGET_COLUMN)\n",
        "\n",
        "test_sequences = np.array(\n",
        "    test_processor.pad_tokenize_sequences(\n",
        "        tokenizer=data_processor.tokenizer,\n",
        "        sequences=test_processor.data[DATA_COLUMN],\n",
        "        max_seq_len=data_processor.max_seq_len,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "LFdLepuLhqIg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Unknown data processing (extended vocabulary)\n",
        "# ============================================================\n",
        "unknown_processor = DataProcessor(unknown_data_path, TARGET_COLUMN)\n",
        "\n",
        "temp_tokenizer = Tokenizer()\n",
        "extended_data = pd.concat([data_processor.data, unknown_processor.data])\n",
        "temp_tokenizer.fit_on_texts(extended_data[DATA_COLUMN])\n",
        "\n",
        "unknown_processor.set_tokenizer(\n",
        "    temp_tokenizer,\n",
        "    data_processor.max_seq_len,\n",
        "    data_processor.stats[\"embedding_dim\"],\n",
        ")\n",
        "\n",
        "unknown_processor.length_stats(extended_data[DATA_COLUMN])\n",
        "unknown_processor.print_stats(unknown_processor.stats)\n",
        "\n",
        "unknown_processor.max_seq_len = data_processor.max_seq_len\n",
        "\n",
        "unknown_sequences = np.array(\n",
        "    unknown_processor.pad_tokenize_sequences(\n",
        "        tokenizer=unknown_processor.tokenizer,\n",
        "        sequences=unknown_processor.data[DATA_COLUMN],\n",
        "        max_seq_len=unknown_processor.max_seq_len,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "GIDLkm0bhqFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Dataset merging & splitting\n",
        "# ============================================================\n",
        "merged_sequences = np.concatenate((test_sequences, unknown_sequences), axis=0)\n",
        "merged_targets = pd.concat([test_processor.targets, unknown_processor.targets])\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(\n",
        "    data_sequences,\n",
        "    data_processor.targets,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        ")"
      ],
      "metadata": {
        "id": "qU5ZVIf6hqCc"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Target scaling\n",
        "# ============================================================\n",
        "scaler_y = MinMaxScaler()\n",
        "scaler_unknown_y = MinMaxScaler()\n",
        "scaler_merged_y = MinMaxScaler()\n",
        "\n",
        "scaled_train_y = scaler_y.fit_transform(train_y.values.reshape(-1, 1))\n",
        "scaled_val_y   = scaler_y.transform(val_y.values.reshape(-1, 1))\n",
        "scaled_test_y  = scaler_y.transform(test_processor.targets.values.reshape(-1, 1))\n",
        "\n",
        "scaled_unknown_y = scaler_unknown_y.fit_transform(\n",
        "    unknown_processor.targets.values.reshape(-1, 1)\n",
        ")\n",
        "scaled_merged_y = scaler_merged_y.fit_transform(\n",
        "    merged_targets.values.reshape(-1, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "xtpXfcVjdgiO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(history) -> None:\n",
        "    \"\"\"\n",
        "    Plot training (and optionally validation) loss from a Keras History object.\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        raise ValueError(\"Cannot plot learning curve: `history` is None (model not trained?).\")\n",
        "\n",
        "    if not hasattr(history, \"history\") or \"loss\" not in history.history:\n",
        "        raise ValueError(\"Cannot plot learning curve: invalid History object (missing `history['loss']`).\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "\n",
        "    val_loss = history.history.get(\"val_loss\")\n",
        "    if val_loss is not None:\n",
        "        plt.plot(val_loss, label=\"Validation Loss\")\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wHxMIKEDdgfU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Scatter plot of actual vs. predicted values with an ideal y=x reference line.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, color=\"blue\", label=\"Predicted\")\n",
        "    plt.plot(\n",
        "        [y_true.min(), y_true.max()],\n",
        "        [y_true.min(), y_true.max()],\n",
        "        color=\"red\",\n",
        "        linestyle=\"--\",\n",
        "        label=\"Ideal Line\",\n",
        "    )\n",
        "    plt.xlabel(\"Actual\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Actual vs. Predicted Values\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TCQJAvjvdgZT"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute standard regression metrics (MSE, MAE, R2, mean pinball loss).\n",
        "\n",
        "    Returns:\n",
        "        (mse, mae, r2, mpl) or (None, None, None, None) if y_pred contains NaNs.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "\n",
        "    if np.isnan(y_pred).any():\n",
        "        print(\"Evaluation metrics cannot be calculated because y_pred contains NaN values.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mpl = mean_pinball_loss(y_true, y_pred)\n",
        "\n",
        "    return mse, mae, r2, mpl"
      ],
      "metadata": {
        "id": "lHue5VJhdgWZ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_data(model, data: np.ndarray, actual: np.ndarray, scaler) -> None:\n",
        "    \"\"\"\n",
        "    Run model prediction, inverse-transform outputs, print metrics, and plot predictions.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(data)\n",
        "    unscaled_predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "    mse, mae, r2, mpl = evaluate_model(actual, unscaled_predictions)\n",
        "    print(f\"MSE: {mse}\\nMAE: {mae}\\nR2: {r2}\\nMPL: {mpl}\")\n",
        "\n",
        "    plot_predictions(actual, unscaled_predictions)"
      ],
      "metadata": {
        "id": "GdE93MH-kEUi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Shared modules (re-used across model components)\n",
        "# ============================================================\n",
        "# Notes\n",
        "# -----\n",
        "# The number of shared RNN layers is a design choice. In this work, three\n",
        "# stacked RNN-based layers are used as a practical trade-off between model\n",
        "# capacity and training stability. While additional recurrent layers can be\n",
        "# introduced if required, empirical observations suggest that using more than\n",
        "# three recurrent layers (GRUs, LSTMs, etc.) often yields diminishing returns\n",
        "# and may negatively affect optimisation and generalisation, particularly when\n",
        "# the architecture design process is manual.\n",
        "\n",
        "embedding_dim = int(data_processor.stats[\"embedding_dim\"])\n",
        "\n",
        "shared_module_1 = layers.SimpleRNN(embedding_dim, return_sequences=True, name=\"shared_module_1\")\n",
        "shared_module_2 = layers.SimpleRNN(embedding_dim, return_sequences=True, name=\"shared_module_2\")\n",
        "shared_module_3 = layers.SimpleRNN(embedding_dim, return_sequences=True, name=\"shared_module_3\")\n",
        "\n",
        "out_act_fn = \"linear\""
      ],
      "metadata": {
        "id": "5BJkdiykkERn"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Progressive Neural Network\n",
        "# Block 1\n",
        "# ============================================================\n",
        "# NOTE:\n",
        "# This study employs a progressive neural network paradigm.\n",
        "# The number of blocks (columns) and their configurations may vary\n",
        "# depending on the experimental setup. Block 1 serves as the\n",
        "# initial baseline column and is trained independently.\n",
        "#\n",
        "# Optional layers (BatchNorm / Dropout) are intentionally commented\n",
        "# out to preserve the baseline architecture and enable controlled\n",
        "# ablation studies when activated.\n",
        "\n",
        "loss_B1 = \"mse\"\n",
        "optimizer_B1 = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "metrics_list_B1 = [\n",
        "    metrics.MeanAbsoluteError(),\n",
        "    metrics.MeanAbsolutePercentageError(),\n",
        "    metrics.RootMeanSquaredError(),\n",
        "    metrics.MeanSquaredLogarithmicError(),\n",
        "]"
      ],
      "metadata": {
        "id": "sRA0j7qzkEPU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Input & Embedding\n",
        "# ============================================================\n",
        "embedding_layer_B1 = layers.Embedding(\n",
        "    input_dim=data_processor.stats[\"num_words\"],\n",
        "    output_dim=data_processor.stats[\"embedding_dim\"],\n",
        "    trainable=True,\n",
        "    mask_zero=True,\n",
        "    name=\"embedding_layer_B1\",\n",
        ")\n",
        "\n",
        "inp_seq_B1 = layers.Input(\n",
        "    shape=(data_processor.max_seq_len,),\n",
        "    name=\"input_sequence_B1\",\n",
        ")\n",
        "\n",
        "x_B1 = embedding_layer_B1(inp_seq_B1)"
      ],
      "metadata": {
        "id": "3adnmKDakEMR"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Recurrent feature extraction (shared modules)\n",
        "# ============================================================\n",
        "x_B1 = shared_module_1(x_B1)\n",
        "\n",
        "# --- Optional regularisation (disabled for baseline) ---\n",
        "# x_B1 = layers.BatchNormalization(name=\"batch_norm_B1_1\")(x_B1)\n",
        "# x_B1 = layers.Dropout(0.2, name=\"dropout_B1_1\")(x_B1)\n",
        "\n",
        "x_B1 = shared_module_2(x_B1)\n",
        "\n",
        "# --- Optional regularisation (disabled for baseline) ---\n",
        "# x_B1 = layers.BatchNormalization(name=\"batch_norm_B1_2\")(x_B1)\n",
        "# x_B1 = layers.Dropout(0.2, name=\"dropout_B1_2\")(x_B1)\n",
        "\n",
        "x_B1 = shared_module_3(x_B1)\n",
        "\n",
        "# --- Optional regularisation (disabled for baseline) ---\n",
        "# x_B1 = layers.BatchNormalization(name=\"batch_norm_B1_3\")(x_B1)\n",
        "# x_B1 = layers.Dropout(0.2, name=\"dropout_B1_3\")(x_B1)"
      ],
      "metadata": {
        "id": "k07900zlkEJ8"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Aggregation & prediction head\n",
        "# ============================================================\n",
        "x_B1 = layers.GlobalAveragePooling1D(\n",
        "    name=\"global_average_pooling_B1\"\n",
        ")(x_B1)\n",
        "\n",
        "x_B1 = layers.Dense(\n",
        "    int(data_processor.stats[\"embedding_dim\"]),\n",
        "    activation=\"relu\",\n",
        "    name=\"dense_B1_1\",\n",
        ")(x_B1)\n",
        "\n",
        "x_B1 = layers.Dense(\n",
        "    int(data_processor.stats[\"embedding_dim\"]),\n",
        "    activation=\"relu\",\n",
        "    name=\"dense_B1_2\",\n",
        ")(x_B1)\n",
        "\n",
        "out_B1 = layers.Dense(\n",
        "    1,\n",
        "    activation=out_act_fn,\n",
        "    name=\"output_B1\",\n",
        ")(x_B1)"
      ],
      "metadata": {
        "id": "j-5uYm44kEHC"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Model compilation & training\n",
        "# ============================================================\n",
        "model_B1 = Model(\n",
        "    inputs=inp_seq_B1,\n",
        "    outputs=out_B1,\n",
        "    name=\"model_B1\",\n",
        ")\n",
        "\n",
        "model_B1.compile(\n",
        "    loss=loss_B1,\n",
        "    optimizer=optimizer_B1,\n",
        "    metrics=metrics_list_B1,\n",
        ")\n",
        "\n",
        "history_B1 = model_B1.fit(\n",
        "    train_X,\n",
        "    scaled_train_y,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(val_X, scaled_val_y),\n",
        ")\n",
        "\n",
        "plot_learning_curve(history_B1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rILtiobckEEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Evaluation (Block 1)\n",
        "# NOTE:\n",
        "# Block 1's embedding layer was built using the training tokenizer\n",
        "# (`data_processor.tokenizer`). Therefore, any inference inputs must\n",
        "# be tokenized with the same tokenizer to avoid out-of-vocabulary\n",
        "# indices exceeding the embedding `input_dim`.\n",
        "# ============================================================\n",
        "\n",
        "# --- Test (already tokenized with training tokenizer) ---\n",
        "predict_data(model_B1, test_sequences, test_processor.targets, scaler_y)\n",
        "\n",
        "# --- Unknown (re-tokenize with Block 1 tokenizer) ---\n",
        "unknown_sequences_B1 = np.array(\n",
        "    unknown_processor.pad_tokenize_sequences(\n",
        "        tokenizer=data_processor.tokenizer,\n",
        "        sequences=unknown_processor.data[DATA_COLUMN],\n",
        "        max_seq_len=data_processor.max_seq_len,\n",
        "    )\n",
        ")\n",
        "predict_data(model_B1, unknown_sequences_B1, unknown_processor.targets, scaler_unknown_y)\n",
        "\n",
        "# --- Merged (must use the Block 1-compatible unknown sequences) ---\n",
        "merged_sequences_B1 = np.concatenate((test_sequences, unknown_sequences_B1), axis=0)\n",
        "predict_data(model_B1, merged_sequences_B1, merged_targets, scaler_merged_y)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rW5eWbPskEBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Progressive Neural Network\n",
        "# Block 2 (Unknown Data)\n",
        "# ============================================================\n",
        "# NOTE:\n",
        "# Block 2 expands/aligns the embedding vocabulary to the tokenizer fit on\n",
        "# (training + unknown). Therefore, any inference inputs for Block 2 must be\n",
        "# tokenized with `unknown_processor.tokenizer` to avoid index mismatches.\n",
        "\n",
        "# Freeze Block 1 (baseline column)\n",
        "for layer in model_B1.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "7ltAK9r7kD8i"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Optimisation config\n",
        "# ============================================================\n",
        "loss_B2 = \"mse\"\n",
        "optimizer_B2 = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "metrics_list_B2 = [\n",
        "    metrics.MeanAbsoluteError(),\n",
        "    metrics.MeanAbsolutePercentageError(),\n",
        "    metrics.RootMeanSquaredError(),\n",
        "    metrics.MeanSquaredLogarithmicError(),\n",
        "]"
      ],
      "metadata": {
        "id": "4avhBdiBrCr2"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Embedding initialisation (extend pretrained embedding)\n",
        "# ============================================================\n",
        "pretrained_embedding_B2 = embedding_layer_B1.get_weights()[0]\n",
        "\n",
        "temp_embeddings = np.random.uniform(\n",
        "    -0.1,\n",
        "    0.1,\n",
        "    size=(unknown_processor.stats[\"num_words\"], unknown_processor.stats[\"embedding_dim\"]),\n",
        ")\n",
        "\n",
        "# Copy overlapping indices from the pretrained embedding (Block 1)\n",
        "for old_idx in range(min(data_processor.stats[\"num_words\"], unknown_processor.stats[\"num_words\"])):\n",
        "    temp_embeddings[old_idx] = pretrained_embedding_B2[old_idx]\n",
        "\n",
        "embedding_layer_B2 = layers.Embedding(\n",
        "    input_dim=unknown_processor.stats[\"num_words\"],\n",
        "    output_dim=unknown_processor.stats[\"embedding_dim\"],\n",
        "    trainable=True,\n",
        "    mask_zero=True,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(temp_embeddings),\n",
        "    name=\"embedding_layer_B2\",\n",
        ")"
      ],
      "metadata": {
        "id": "Lt0xQEm2rCo9"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Model definition\n",
        "# ============================================================\n",
        "inp_seq_B2 = layers.Input(shape=(unknown_processor.max_seq_len,), name=\"input_sequence_B2\")\n",
        "x_B2 = embedding_layer_B2(inp_seq_B2)\n",
        "\n",
        "# --- Optional: standalone RNN before shared modules (disabled by default) ---\n",
        "# x_B2 = layers.SimpleRNN(int(unknown_processor.stats[\"embedding_dim\"]), return_sequences=True, name=\"rnn_B2_1\")(x_B2)\n",
        "\n",
        "x_B2 = shared_module_1(x_B2)\n",
        "\n",
        "# --- Optional regularisation (disabled for baseline consistency) ---\n",
        "# x_B2 = layers.BatchNormalization(name=\"shared_batch_norm_B2_1\")(x_B2)\n",
        "# x_B2 = layers.Dropout(0.2, name=\"shared_dropout_B2_1\")(x_B2)\n",
        "\n",
        "x_B2 = shared_module_2(x_B2)\n",
        "\n",
        "# --- Optional: additional RNN / regularisation (disabled) ---\n",
        "# x_B2 = layers.SimpleRNN(int(unknown_processor.stats[\"embedding_dim\"]), return_sequences=True, name=\"rnn_B2_2\")(x_B2)\n",
        "# x_B2 = layers.BatchNormalization(name=\"batch_norm_B2_1\")(x_B2)\n",
        "# x_B2 = layers.Dropout(0.2, name=\"dropout_B2_1\")(x_B2)\n",
        "\n",
        "x_B2 = shared_module_3(x_B2)\n",
        "\n",
        "# --- Optional regularisation (disabled) ---\n",
        "# x_B2 = layers.BatchNormalization(name=\"shared_batch_norm_B2_2\")(x_B2)\n",
        "# x_B2 = layers.Dropout(0.2, name=\"shared_dropout_B2_2\")(x_B2)\n",
        "\n",
        "x_B2 = layers.GlobalAveragePooling1D(name=\"global_average_pooling_B2\")(x_B2)\n",
        "\n",
        "x_B2 = layers.Dense(int(unknown_processor.stats[\"embedding_dim\"]), activation=\"relu\", name=\"dense_B2_1\")(x_B2)\n",
        "x_B2 = layers.Dense(int(unknown_processor.stats[\"embedding_dim\"]), activation=\"relu\", name=\"dense_B2_2\")(x_B2)\n",
        "\n",
        "out_B2 = layers.Dense(1, activation=out_act_fn, name=\"output_B2\")(x_B2)\n",
        "\n",
        "model_B2 = Model(inputs=inp_seq_B2, outputs=out_B2, name=\"model_B2\")\n",
        "model_B2.compile(loss=loss_B2, optimizer=optimizer_B2, metrics=metrics_list_B2)"
      ],
      "metadata": {
        "id": "yJUmiew6rCmH"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Training (unknown data)\n",
        "# ============================================================\n",
        "history_B2 = model_B2.fit(\n",
        "    unknown_sequences,\n",
        "    scaled_unknown_y,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        ")\n",
        "plot_learning_curve(history_B2)"
      ],
      "metadata": {
        "id": "ZjleWM62rCjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Evaluation (Block 2)\n",
        "# NOTE:\n",
        "# Block 2 expects token IDs produced by `unknown_processor.tokenizer`.\n",
        "# Therefore, test data must be re-tokenized under the unknown tokenizer.\n",
        "# ============================================================\n",
        "\n",
        "# Re-tokenize test data for Block 2 inference (unknown tokenizer)\n",
        "test_sequences_B2 = np.array(\n",
        "    test_processor.pad_tokenize_sequences(\n",
        "        tokenizer=unknown_processor.tokenizer,\n",
        "        sequences=test_processor.data[DATA_COLUMN],\n",
        "        max_seq_len=unknown_processor.max_seq_len,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "predict_data(model_B2, test_sequences_B2, test_processor.targets, scaler_y)\n",
        "predict_data(model_B2, unknown_sequences, unknown_processor.targets, scaler_unknown_y)\n",
        "\n",
        "merged_sequences_B2 = np.concatenate((test_sequences_B2, unknown_sequences), axis=0)\n",
        "predict_data(model_B2, merged_sequences_B2, merged_targets, scaler_merged_y)"
      ],
      "metadata": {
        "id": "yB_r-Na7rCf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Progressive Neural Network\n",
        "# Block 3 (Merged Data)\n",
        "# ============================================================\n",
        "# NOTE:\n",
        "# Block 3 is trained on merged data (test + unknown). Since it uses the\n",
        "# tokenizer/vocabulary defined for the unknown-extended setting, all inputs\n",
        "# must be tokenized with `unknown_processor.tokenizer`.\n",
        "\n",
        "loss_B3 = \"mse\"\n",
        "optimizer_B3 = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "metrics_list_B3 = [\n",
        "    metrics.MeanAbsoluteError(),\n",
        "    metrics.MeanAbsolutePercentageError(),\n",
        "    metrics.RootMeanSquaredError(),\n",
        "    metrics.MeanSquaredLogarithmicError(),\n",
        "]"
      ],
      "metadata": {
        "id": "-4t_mE9zrOIF"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Embedding initialisation (extend pretrained embedding from Block 1)\n",
        "# ============================================================\n",
        "pretrained_embedding_B3 = embedding_layer_B1.get_weights()[0]\n",
        "\n",
        "temp_embeddings = np.random.uniform(\n",
        "    -0.1,\n",
        "    0.1,\n",
        "    size=(unknown_processor.stats[\"num_words\"], unknown_processor.stats[\"embedding_dim\"]),\n",
        ")\n",
        "\n",
        "for old_idx in range(min(data_processor.stats[\"num_words\"], unknown_processor.stats[\"num_words\"])):\n",
        "    temp_embeddings[old_idx] = pretrained_embedding_B3[old_idx]\n",
        "\n",
        "embedding_layer_B3 = layers.Embedding(\n",
        "    input_dim=unknown_processor.stats[\"num_words\"],\n",
        "    output_dim=unknown_processor.stats[\"embedding_dim\"],\n",
        "    trainable=True,\n",
        "    mask_zero=True,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(temp_embeddings),\n",
        "    name=\"embedding_layer_B3\",\n",
        ")"
      ],
      "metadata": {
        "id": "DwGkFyP7rOFd"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Model definition\n",
        "# ============================================================\n",
        "inp_seq_B3 = layers.Input(\n",
        "    shape=(unknown_processor.max_seq_len,),\n",
        "    name=\"input_sequence_B3\",  # fixed name\n",
        ")\n",
        "\n",
        "# Use the embedding layer defined for B3 (fixed)\n",
        "x_B3 = embedding_layer_B3(inp_seq_B3)\n",
        "\n",
        "x_B3 = shared_module_1(x_B3)\n",
        "\n",
        "# --- Optional regularisation (disabled) ---\n",
        "# x_B3 = layers.BatchNormalization(name=\"shared_batch_norm_B3_1\")(x_B3)\n",
        "# x_B3 = layers.Dropout(0.2, name=\"shared_dropout_B3_1\")(x_B3)\n",
        "\n",
        "x_B3 = shared_module_2(x_B3)\n",
        "\n",
        "# --- Optional: extra RNN / regularisation (disabled) ---\n",
        "# x_B3 = layers.SimpleRNN(int(unknown_processor.stats[\"embedding_dim\"]), return_sequences=True, name=\"rnn_B3_1\")(x_B3)\n",
        "# x_B3 = layers.BatchNormalization(name=\"batch_norm_B3_1\")(x_B3)\n",
        "# x_B3 = layers.Dropout(0.2, name=\"dropout_B3_1\")(x_B3)\n",
        "\n",
        "x_B3 = shared_module_3(x_B3)\n",
        "\n",
        "# --- Optional regularisation (disabled) ---\n",
        "# x_B3 = layers.BatchNormalization(name=\"shared_batch_norm_B3_2\")(x_B3)\n",
        "# x_B3 = layers.Dropout(0.2, name=\"shared_dropout_B3_2\")(x_B3)\n",
        "\n",
        "x_B3 = layers.GlobalAveragePooling1D(name=\"global_average_pooling_B3\")(x_B3)\n",
        "\n",
        "x_B3 = layers.Dense(int(unknown_processor.stats[\"embedding_dim\"]), activation=\"relu\", name=\"dense_B3_1\")(x_B3)\n",
        "x_B3 = layers.Dense(int(unknown_processor.stats[\"embedding_dim\"]), activation=\"relu\", name=\"dense_B3_2\")(x_B3)\n",
        "\n",
        "out_B3 = layers.Dense(1, activation=out_act_fn, name=\"output_B3\")(x_B3)\n",
        "\n",
        "model_B3 = Model(inputs=inp_seq_B3, outputs=out_B3, name=\"model_B3\")\n",
        "model_B3.compile(loss=loss_B3, optimizer=optimizer_B3, metrics=metrics_list_B3)"
      ],
      "metadata": {
        "id": "GZVNNDHZrOCw"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Train/val split on merged data (tokenized with unknown tokenizer)\n",
        "# ============================================================\n",
        "# Use the merged sequences that are compatible with the unknown tokenizer.\n",
        "# If you followed Block 2 refactor, this is `merged_sequences_B2`.\n",
        "merged_sequences_for_B3 = merged_sequences_B2  # alias for clarity\n",
        "\n",
        "merged_train_X, merged_val_X, merged_train_y, merged_val_y = train_test_split(\n",
        "    merged_sequences_for_B3,\n",
        "    scaled_merged_y,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "history_B3 = model_B3.fit(\n",
        "    merged_train_X,\n",
        "    merged_train_y,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(merged_val_X, merged_val_y),\n",
        ")\n",
        "plot_learning_curve(history_B3)"
      ],
      "metadata": {
        "id": "ZwkiexplrCc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Evaluation (Block 3)\n",
        "# ============================================================\n",
        "# Block 3 expects unknown-tokenizer sequences, so use the B2-compatible test/merged.\n",
        "predict_data(model_B3, test_sequences_B2, test_processor.targets, scaler_y)\n",
        "predict_data(model_B3, unknown_sequences, unknown_processor.targets, scaler_unknown_y)\n",
        "predict_data(model_B3, merged_sequences_for_B3, merged_targets, scaler_merged_y)"
      ],
      "metadata": {
        "id": "hSQIPWl1mjgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO58NMAQfnGGWi6n9XIgD/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}